<!DOCTYPE html> 
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MemoLearning Support Vector Machines</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      background: linear-gradient(135deg, #7c3aed 0%, #6d28d9 50%, #5b21b6 100%);
      color: #111827;
      margin: 0;
      padding: 0;
      min-height: 100vh;
    }
    
    header {
      background: rgba(255, 255, 255, 0.1);
      backdrop-filter: blur(20px);
      color: white;
      padding: 40px 20px;
      text-align: center;
      border-bottom: 1px solid rgba(255, 255, 255, 0.2);
    }
    
    header h1 {
      font-size: 42px;
      margin: 0;
      font-weight: 800;
      text-shadow: 0 2px 20px rgba(0,0,0,0.3);
    }
    
    header p {
      margin-top: 15px;
      font-size: 18px;
      opacity: 0.9;
    }
    
    .back-link {
      display: inline-block;
      margin-top: 25px;
      padding: 12px 24px;
      background: rgba(255, 255, 255, 0.2);
      color: white;
      text-decoration: none;
      border-radius: 50px;
      font-weight: 600;
      backdrop-filter: blur(10px);
      border: 1px solid rgba(255, 255, 255, 0.3);
      transition: all 0.3s ease;
    }
    
    .back-link:hover {
      background: rgba(255, 255, 255, 0.3);
      transform: translateY(-2px);
      box-shadow: 0 10px 25px rgba(0,0,0,0.2);
    }
    
    .container {
      padding: 40px 20px;
      max-width: 1400px;
      margin: auto;
    }
    
    .units-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
      gap: 30px;
      margin-top: 20px;
    }
    
    .unit-card {
      background: rgba(255, 255, 255, 0.95);
      border-radius: 20px;
      padding: 30px;
      box-shadow: 0 15px 35px rgba(0,0,0,0.1);
      backdrop-filter: blur(20px);
      border: 1px solid rgba(255, 255, 255, 0.5);
      transition: all 0.4s ease;
      cursor: pointer;
      position: relative;
      overflow: hidden;
    }
    
    .unit-card::before {
      content: '';
      position: absolute;
      top: 0;
      left: -100%;
      width: 100%;
      height: 100%;
      background: linear-gradient(90deg, transparent, rgba(255,255,255,0.4), transparent);
      transition: left 0.5s;
    }
    
    .unit-card:hover::before {
      left: 100%;
    }
    
    .unit-card:hover {
      transform: translateY(-10px) scale(1.02);
      box-shadow: 0 25px 50px rgba(0,0,0,0.15);
      background: rgba(255, 255, 255, 1);
    }
    
    .unit-header {
      display: flex;
      align-items: center;
      margin-bottom: 20px;
    }
    
    .unit-number {
      background: linear-gradient(135deg, #7c3aed, #6d28d9);
      color: white;
      width: 40px;
      height: 40px;
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      font-weight: 800;
      margin-right: 15px;
      box-shadow: 0 4px 15px rgba(124, 58, 237, 0.4);
    }
    
    .unit-title {
      font-size: 24px;
      font-weight: 800;
      color: #1f2937;
      margin: 0;
      flex: 1;
    }
    
    .unit-description {
      color: #6b7280;
      font-size: 16px;
      margin-bottom: 20px;
      line-height: 1.6;
    }
    
    .topics-list {
      list-style: none;
      padding: 0;
      margin: 0;
    }
    
    .topics-list li {
      padding: 8px 0;
      border-bottom: 1px solid #f3f4f6;
      color: #374151;
      position: relative;
      padding-left: 20px;
    }
    
    .topics-list li:before {
      content: 'üéØ';
      position: absolute;
      left: 0;
      font-size: 12px;
    }
    
    .topics-list li:last-child {
      border-bottom: none;
    }
    
    .curriculum-stats {
      background: rgba(255, 255, 255, 0.1);
      backdrop-filter: blur(20px);
      border-radius: 20px;
      padding: 30px;
      margin-bottom: 40px;
      text-align: center;
      color: white;
    }
    
    .stats-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 30px;
      margin-top: 20px;
    }
    
    .stat-item {
      text-align: center;
    }
    
    .stat-number {
      font-size: 36px;
      font-weight: 800;
      display: block;
    }
    
    .stat-label {
      font-size: 14px;
      opacity: 0.8;
      margin-top: 5px;
    }

    .page-container {
      display: none;
      padding: 40px 20px;
      max-width: 1200px;
      margin: auto;
      background: rgba(255, 255, 255, 0.95);
      border-radius: 20px;
      margin-top: 20px;
      box-shadow: 0 15px 35px rgba(0,0,0,0.1);
    }

    .page-container.active {
      display: block;
    }

    .unit-detail-header {
      text-align: center;
      margin-bottom: 40px;
      padding-bottom: 20px;
      border-bottom: 2px solid #e5e7eb;
    }

    .unit-detail-title {
      font-size: 32px;
      font-weight: 800;
      color: #1f2937;
      margin-bottom: 10px;
    }

    .subtopics-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 20px;
    }

    .subtopic-card {
      background: white;
      border: 2px solid #e5e7eb;
      border-radius: 12px;
      padding: 20px;
      transition: all 0.3s ease;
    }

    .subtopic-card:hover {
      border-color: #7c3aed;
      transform: translateY(-2px);
      box-shadow: 0 8px 25px rgba(0,0,0,0.1);
    }

    .subtopic-title {
      font-size: 18px;
      font-weight: 600;
      color: #6d28d9;
      margin-bottom: 10px;
    }

    .back-to-overview {
      background: linear-gradient(135deg, #7c3aed, #6d28d9);
      color: white;
      border: none;
      padding: 12px 24px;
      border-radius: 25px;
      font-weight: 600;
      cursor: pointer;
      margin-bottom: 30px;
      transition: all 0.3s ease;
    }

    .back-to-overview:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 20px rgba(124, 58, 237, 0.3);
    }

    .code-example {
      background: #f8fafc;
      border: 1px solid #e2e8f0;
      border-radius: 8px;
      padding: 15px;
      margin: 10px 0;
      font-family: 'Courier New', monospace;
      font-size: 14px;
      color: #334155;
    }

    .svm-badge {
      display: inline-block;
      background: #f3e8ff;
      color: #5b21b6;
      padding: 4px 8px;
      border-radius: 12px;
      font-size: 12px;
      font-weight: 600;
      margin: 2px;
    }

    .margin-box {
      background: #faf5ff;
      border: 1px solid #d8b4fe;
      border-radius: 8px;
      padding: 15px;
      margin: 10px 0;
      font-size: 14px;
      color: #5b21b6;
    }

    .kernel-highlight {
      background: #f0f9ff;
      border: 1px solid #bae6fd;
      border-left: 4px solid #0ea5e9;
      border-radius: 8px;
      padding: 15px;
      margin: 10px 0;
      font-size: 14px;
      color: #0c4a6e;
    }

    .math-formula {
      background: #fef3c7;
      border: 1px solid #fed7aa;
      border-radius: 8px;
      padding: 15px;
      margin: 10px 0;
      font-family: 'Times New Roman', serif;
      font-size: 16px;
      text-align: center;
      color: #92400e;
    }
  </style>
  <script>
    function showUnitDetail(unitNumber) {
      // Hide overview
      document.getElementById('overview').style.display = 'none';
      
      // Hide all unit detail pages
      const allUnits = document.querySelectorAll('.page-container');
      allUnits.forEach(unit => unit.classList.remove('active'));
      
      // Show selected unit
      const selectedUnit = document.getElementById('unit-' + unitNumber);
      if (selectedUnit) {
        selectedUnit.classList.add('active');
      }
    }

    function showOverview() {
      // Hide all unit detail pages
      const allUnits = document.querySelectorAll('.page-container');
      allUnits.forEach(unit => unit.classList.remove('active'));
      
      // Show overview
      document.getElementById('overview').style.display = 'block';
    }
  </script>
</head>
<body>
  <header>
    <h1>üéØ Support Vector Machines</h1>
    <p>Master maximum margin classifiers, kernel methods, and optimization techniques for robust pattern recognition</p>
    <a class="back-link" href="#" onclick="alert('This would navigate back to data science courses')">‚Üê Back to Data Science</a>
  </header>

  <div class="container" id="overview">
    <div class="curriculum-stats">
      <h2 style="margin: 0 0 20px 0; font-size: 28px;">Support Vector Machines Curriculum</h2>
      <div class="stats-grid">
        <div class="stat-item">
          <span class="stat-number">11</span>
          <div class="stat-label">Core Units</div>
        </div>
        <div class="stat-item">
          <span class="stat-number">~65</span>
          <div class="stat-label">Key Concepts</div>
        </div>
        <div class="stat-item">
          <span class="stat-number">8+</span>
          <div class="stat-label">Kernel Types</div>
        </div>
        <div class="stat-item">
          <span class="stat-number">25+</span>
          <div class="stat-label">Practical Examples</div>
        </div>
      </div>
    </div>

    <div class="units-grid">
      <div class="unit-card" onclick="showUnitDetail(1)">
        <div class="unit-header">
          <div class="unit-number">1</div>
          <h3 class="unit-title">Linear Separable Classification</h3>
        </div>
        <p class="unit-description">Understand linear separability and the foundation of support vector machines.</p>
        <ul class="topics-list">
          <li>Linear separability concept</li>
          <li>Hyperplane geometry</li>
          <li>Decision boundaries</li>
          <li>Perceptron limitations</li>
          <li>Multiple separating planes</li>
          <li>Optimal hyperplane idea</li>
          <li>Binary classification setup</li>
          <li>Mathematical formulation</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(2)">
        <div class="unit-header">
          <div class="unit-number">2</div>
          <h3 class="unit-title">Maximum Margin Principle</h3>
        </div>
        <p class="unit-description">Learn the core principle of maximizing margin for optimal classification.</p>
        <ul class="topics-list">
          <li>Margin definition</li>
          <li>Geometric margin</li>
          <li>Functional margin</li>
          <li>Maximum margin classifier</li>
          <li>Distance to hyperplane</li>
          <li>Margin maximization intuition</li>
          <li>Generalization benefits</li>
          <li>Unique optimal solution</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(3)">
        <div class="unit-header">
          <div class="unit-number">3</div>
          <h3 class="unit-title">Support Vectors</h3>
        </div>
        <p class="unit-description">Understand the crucial role of support vectors in SVM optimization.</p>
        <ul class="topics-list">
          <li>Support vector definition</li>
          <li>Critical data points</li>
          <li>Margin boundaries</li>
          <li>Sparse solution property</li>
          <li>Support vector identification</li>
          <li>Model robustness</li>
          <li>Outlier resistance</li>
          <li>Computational efficiency</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(4)">
        <div class="unit-header">
          <div class="unit-number">4</div>
          <h3 class="unit-title">Optimization Problem</h3>
        </div>
        <p class="unit-description">Master the mathematical optimization framework behind SVMs.</p>
        <ul class="topics-list">
          <li>Primal optimization problem</li>
          <li>Quadratic programming</li>
          <li>Lagrange multipliers</li>
          <li>KKT conditions</li>
          <li>Dual formulation</li>
          <li>Convex optimization</li>
          <li>Global optimum guarantee</li>
          <li>SMO algorithm</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(5)">
        <div class="unit-header">
          <div class="unit-number">5</div>
          <h3 class="unit-title">Soft Margin SVM</h3>
        </div>
        <p class="unit-description">Handle non-separable data with soft margin classification.</p>
        <ul class="topics-list">
          <li>Non-separable data problems</li>
          <li>Slack variables introduction</li>
          <li>Soft margin formulation</li>
          <li>C parameter tuning</li>
          <li>Bias-variance tradeoff</li>
          <li>Hinge loss function</li>
          <li>Regularization effects</li>
          <li>Outlier handling</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(6)">
        <div class="unit-header">
          <div class="unit-number">6</div>
          <h3 class="unit-title">Kernel Trick</h3>
        </div>
        <p class="unit-description">Transform data into higher dimensions using the powerful kernel trick.</p>
        <ul class="topics-list">
          <li>Feature space transformation</li>
          <li>High-dimensional mapping</li>
          <li>Kernel function concept</li>
          <li>Dot product in feature space</li>
          <li>Computational efficiency</li>
          <li>Mercer's theorem</li>
          <li>Valid kernel conditions</li>
          <li>Implicit feature mapping</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(7)">
        <div class="unit-header">
          <div class="unit-number">7</div>
          <h3 class="unit-title">Kernel Functions</h3>
        </div>
        <p class="unit-description">Explore different kernel functions and their applications.</p>
        <ul class="topics-list">
          <li>Linear kernel</li>
          <li>Polynomial kernel</li>
          <li>Radial Basis Function (RBF)</li>
          <li>Gaussian kernel</li>
          <li>Sigmoid kernel</li>
          <li>Custom kernel design</li>
          <li>Kernel parameter tuning</li>
          <li>Kernel selection strategies</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(8)">
        <div class="unit-header">
          <div class="unit-number">8</div>
          <h3 class="unit-title">Non-linear Classification</h3>
        </div>
        <p class="unit-description">Apply SVMs to complex non-linear classification problems.</p>
        <ul class="topics-list">
          <li>Non-linear decision boundaries</li>
          <li>Kernel-based classification</li>
          <li>Feature space visualization</li>
          <li>RBF kernel applications</li>
          <li>Polynomial relationships</li>
          <li>Complex pattern recognition</li>
          <li>Overfitting prevention</li>
          <li>Kernel parameter effects</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(9)">
        <div class="unit-header">
          <div class="unit-number">9</div>
          <h3 class="unit-title">SVM for Regression</h3>
        </div>
        <p class="unit-description">Extend SVM principles to regression problems with SVR.</p>
        <ul class="topics-list">
          <li>Support Vector Regression</li>
          <li>Epsilon-insensitive loss</li>
          <li>Tube regression concept</li>
          <li>Linear SVR</li>
          <li>Non-linear SVR</li>
          <li>Support vector identification</li>
          <li>Hyperparameter tuning</li>
          <li>Robust regression</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(10)">
        <div class="unit-header">
          <div class="unit-number">10</div>
          <h3 class="unit-title">Multiclass SVM</h3>
        </div>
        <p class="unit-description">Extend binary SVMs to handle multiclass classification problems.</p>
        <ul class="topics-list">
          <li>One-vs-One strategy</li>
          <li>One-vs-Rest approach</li>
          <li>Binary decomposition</li>
          <li>Voting mechanisms</li>
          <li>ECOC methods</li>
          <li>Computational complexity</li>
          <li>Decision function combination</li>
          <li>Multiclass optimization</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(11)">
        <div class="unit-header">
          <div class="unit-number">11</div>
          <h3 class="unit-title">Implementation and Practice</h3>
        </div>
        <p class="unit-description">Build and deploy SVM models using practical tools and techniques.</p>
        <ul class="topics-list">
          <li>Scikit-learn SVM implementation</li>
          <li>Data preprocessing for SVM</li>
          <li>Feature scaling importance</li>
          <li>Hyperparameter tuning</li>
          <li>Cross-validation strategies</li>
          <li>Model evaluation metrics</li>
          <li>Computational scalability</li>
          <li>Real-world applications</li>
        </ul>
      </div>
    </div>
  </div>

  <!-- Unit Detail Pages -->
  <div class="page-container" id="unit-1">
    <button class="back-to-overview" onclick="showOverview()">‚Üê Back to Overview</button>
    <div class="unit-detail-header">
      <h1 class="unit-detail-title">Unit 1: Linear Separable Classification</h1>
      <p>Understand linear separability and the foundation of support vector machines.</p>
    </div>
    <div class="subtopics-grid">
      <div class="subtopic-card">
        <h3 class="subtopic-title">Linear Separability Concept</h3>
        <p>Learn what it means for data to be linearly separable and why this is important for SVMs.</p>
        <span class="svm-badge">Hyperplane</span>
        <span class="svm-badge">Separable</span>
        <span class="svm-badge">Binary</span>
        <div class="margin-box">
          A dataset is linearly separable if there exists a hyperplane that can perfectly separate the data points of different classes without any misclassification errors.
        </div>
      </div>
      <div class="subtopic-card">
        <h3 class="subtopic-title">Hyperplane Geometry</h3>
        <p>Understand the mathematical representation of hyperplanes in n-dimensional space.</p>
        <div class="math-formula">
          Hyperplane Equation: w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b = 0<br>
          Or in vector form: w·µÄx + b = 0
        </div>
        <div class="code-example">
          import numpy as np<br>
          import matplotlib.pyplot as plt<br>
          from sklearn.datasets import make_classification<br>
          <br>
          # Generate linearly separable data<br>
          X, y = make_classification(n_samples=100, n_features=2, <br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n_redundant=0, n_informative=2,<br>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n_clusters_per_class=1, random_state=42)<br>
          <br>
          # Hyperplane parameters<br>
          w = np.array([1, -1])  # Normal vector<br>
          b = 0.5                # Bias term<br>
          <br>
          # Plot data and hyperplane<br>
          plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu')<br>
          <br>
          # Create hyperplane<br>
          x_line = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)<br>
          y_line = -(w[0] * x_line + b) / w[1]<br>
          plt.plot(x_line, y_line, 'k-', linewidth=2)<br>
          plt.title('Linear Separable Data with Hyperplane')<br>
          plt.show()
        </div>
      </div>
      <div class="subtopic-card">
        <h3 class="subtopic-title">Decision Boundaries</h3>
        <p>Understand how hyperplanes serve as decision boundaries for classification.</p>
        <div class="kernel-highlight">
          Decision Rule: For a point x, classify as:<br>
          ‚Ä¢ Class +1 if w·µÄx + b > 0<br>
          ‚Ä¢ Class -1 if w·µÄx + b < 0<br>
          ‚Ä¢ On boundary if w·µÄx + b = 0
        </div>
        <div class="code-example">
          def classify_point(x, w, b):<br>
          &nbsp;&nbsp;"""Classify a point using linear decision boundary"""<br>
          &nbsp;&nbsp;decision_value = np.dot(w, x) + b<br>
          &nbsp;&nbsp;<br>
          &nbsp;&nbsp;if decision_value > 0:<br>
          &nbsp;&nbsp;&nbsp;&nbsp;return +1<br>
          &nbsp;&nbsp;elif decision_value < 0:<br>
          &nbsp;&nbsp;&nbsp;&nbsp;return -1<br>
          &nbsp;&nbsp;else:<br>
          &nbsp;&nbsp;&nbsp;&nbsp;return 0  # On the boundary<br>
          <br>
          # Example usage<br>
          w = np.array([1, -1])<br>
          b = 0.5<br>
          <br>
          test_points = np.array([[2, 1], [1, 2], [0, 0.5]])<br>
          <br>
          for point in test_points:<br>
          &nbsp;&nbsp;prediction = classify_point(point, w, b)<br>
          &nbsp;&nbsp;distance = abs(np.dot(w, point) + b) / np.linalg.norm(w)<br>
          &nbsp;&nbsp;print(f"Point {point}: Class {prediction}, Distance {distance:.2f}")
        </div>
      </div>
      <div class="subtopic-card">
        <h3 class="subtopic-title">Multiple Separating Planes</h3>
        <p>Learn why there can be infinite hyperplanes that separate linearly separable data.</p>
        <div class="margin-box">
          For linearly separable data, there are infinitely many hyperplanes that can achieve perfect separation. The question becomes: which one should we choose and why?
        </div>
        <div class="code-example">
          # Demonstrate multiple separating hyperplanes<br>
          import numpy as np<br>
          import matplotlib.pyplot as plt<br>
          <br>
          # Simple 2D linearly separable data<br>
          class1 = np.array([[1, 1], [2, 2], [2, 1]])<br>
          class2 = np.array([[4, 4], [5, 5], [4, 5]])<br>
          <br>
          plt.figure(figsize=(8, 6))<br>
          plt.scatter(class1[:, 0], class1[:, 1], c='red', marker='o', s=100)<br>
          plt.scatter(class2[:, 0], class2[:, 1], c='blue', marker='s', s=100)<br>
          <br>
          # Multiple possible separating lines<br>
          x = np.linspace(0, 6, 100)<br>
          <br>
          # Different separating hyperplanes<br>
          y1 = x - 0.5    # Line 1<br>
          y2 = x + 0.5    # Line 2<br>
          y3 = 0.5*x + 1  # Line 3<br>
          <br>
          plt.plot(x, y1, 'g--', label='Hyperplane 1')<br>
          plt.plot(x, y2, 'm--', label='Hyperplane 2')<br>
          plt.plot(x, y3, 'orange', '--', label='Hyperplane 3')<br>
          <br>
          plt.legend()<br>
          plt.title('Multiple Possible Separating Hyperplanes')<br>
          plt.xlabel('X1')<br>
          plt.ylabel('X2')<br>
          plt.grid(True, alpha=0.3)<br>
          plt.show()
        </div>
      </div>
      <div class="subtopic-card">
        <h3 class="subtopic-title">Optimal Hyperplane Idea</h3>
        <p>Understand the motivation for finding the "best" hyperplane among all possible separating hyperplanes.</p>
        <span class="svm-badge">Generalization</span>
        <span class="svm-badge">Robustness</span>
        <span class="svm-badge">Margin</span>
        <div class="code-example">
          # Intuition: Why maximum margin?<br>
          <br>
          reasons_for_max_margin = {<br>
          &nbsp;&nbsp;"Generalization": "Better performance on unseen data",<br>
          &nbsp;&nbsp;"Robustness": "Less sensitive to small data variations",<br>
          &nbsp;&nbsp;"Confidence": "Points far from boundary are classified more confidently",<br>
          &nbsp;&nbsp;"Uniqueness": "Only one maximum margin hyperplane exists",<br>
          &nbsp;&nbsp;"Theory": "Statistical learning theory supports maximum margin"<br>
          }<br>
          <br>
          # The maximum margin hyperplane maximizes the minimum<br>
          # distance from any training point to the decision boundary<br>
          <br>
          def margin_width(X, y, w, b):<br>
          &nbsp;&

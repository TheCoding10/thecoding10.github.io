<!DOCTYPE html> 
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MemoLearning Dimensionality Reduction</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      background: linear-gradient(135deg, #a855f7 0%, #7c3aed 50%, #5b21b6 100%);
      color: #111827;
      margin: 0;
      padding: 0;
      min-height: 100vh;
    }
    
    header {
      background: rgba(255, 255, 255, 0.1);
      backdrop-filter: blur(20px);
      color: white;
      padding: 40px 20px;
      text-align: center;
      border-bottom: 1px solid rgba(255, 255, 255, 0.2);
    }
    
    header h1 {
      font-size: 42px;
      margin: 0;
      font-weight: 800;
      text-shadow: 0 2px 20px rgba(0,0,0,0.3);
    }
    
    header p {
      margin-top: 15px;
      font-size: 18px;
      opacity: 0.9;
    }
    
    .back-link {
      display: inline-block;
      margin-top: 25px;
      padding: 12px 24px;
      background: rgba(255, 255, 255, 0.2);
      color: white;
      text-decoration: none;
      border-radius: 50px;
      font-weight: 600;
      backdrop-filter: blur(10px);
      border: 1px solid rgba(255, 255, 255, 0.3);
      transition: all 0.3s ease;
    }
    
    .back-link:hover {
      background: rgba(255, 255, 255, 0.3);
      transform: translateY(-2px);
      box-shadow: 0 10px 25px rgba(0,0,0,0.2);
    }
    
    .container {
      padding: 40px 20px;
      max-width: 1400px;
      margin: auto;
    }
    
    .units-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
      gap: 30px;
      margin-top: 20px;
    }
    
    .unit-card {
      background: rgba(255, 255, 255, 0.95);
      border-radius: 20px;
      padding: 30px;
      box-shadow: 0 15px 35px rgba(0,0,0,0.1);
      backdrop-filter: blur(20px);
      border: 1px solid rgba(255, 255, 255, 0.5);
      transition: all 0.4s ease;
      cursor: pointer;
      position: relative;
      overflow: hidden;
    }
    
    .unit-card::before {
      content: '';
      position: absolute;
      top: 0;
      left: -100%;
      width: 100%;
      height: 100%;
      background: linear-gradient(90deg, transparent, rgba(255,255,255,0.4), transparent);
      transition: left 0.5s;
    }
    
    .unit-card:hover::before {
      left: 100%;
    }
    
    .unit-card:hover {
      transform: translateY(-10px) scale(1.02);
      box-shadow: 0 25px 50px rgba(0,0,0,0.15);
      background: rgba(255, 255, 255, 1);
    }
    
    .unit-header {
      display: flex;
      align-items: center;
      margin-bottom: 20px;
    }
    
    .unit-number {
      background: linear-gradient(135deg, #a855f7, #7c3aed);
      color: white;
      width: 40px;
      height: 40px;
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      font-weight: 800;
      margin-right: 15px;
      box-shadow: 0 4px 15px rgba(168, 85, 247, 0.4);
    }
    
    .unit-title {
      font-size: 24px;
      font-weight: 800;
      color: #1f2937;
      margin: 0;
      flex: 1;
    }
    
    .unit-description {
      color: #6b7280;
      font-size: 16px;
      margin-bottom: 20px;
      line-height: 1.6;
    }
    
    .topics-list {
      list-style: none;
      padding: 0;
      margin: 0;
    }
    
    .topics-list li {
      padding: 8px 0;
      border-bottom: 1px solid #f3f4f6;
      color: #374151;
      position: relative;
      padding-left: 20px;
    }
    
    .topics-list li:before {
      content: 'üìê';
      position: absolute;
      left: 0;
      font-size: 12px;
    }
    
    .topics-list li:last-child {
      border-bottom: none;
    }
    
    .curriculum-stats {
      background: rgba(255, 255, 255, 0.1);
      backdrop-filter: blur(20px);
      border-radius: 20px;
      padding: 30px;
      margin-bottom: 40px;
      text-align: center;
      color: white;
    }
    
    .stats-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 30px;
      margin-top: 20px;
    }
    
    .stat-item {
      text-align: center;
    }
    
    .stat-number {
      font-size: 36px;
      font-weight: 800;
      display: block;
    }
    
    .stat-label {
      font-size: 14px;
      opacity: 0.8;
      margin-top: 5px;
    }

    .page-container {
      display: none;
      padding: 40px 20px;
      max-width: 1200px;
      margin: auto;
      background: rgba(255, 255, 255, 0.95);
      border-radius: 20px;
      margin-top: 20px;
      box-shadow: 0 15px 35px rgba(0,0,0,0.1);
    }

    .page-container.active {
      display: block;
    }

    .unit-detail-header {
      text-align: center;
      margin-bottom: 40px;
      padding-bottom: 20px;
      border-bottom: 2px solid #e5e7eb;
    }

    .unit-detail-title {
      font-size: 32px;
      font-weight: 800;
      color: #1f2937;
      margin-bottom: 10px;
    }

    .subtopics-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 20px;
    }

    .subtopic-card {
      background: white;
      border: 2px solid #e5e7eb;
      border-radius: 12px;
      padding: 20px;
      transition: all 0.3s ease;
    }

    .subtopic-card:hover {
      border-color: #a855f7;
      transform: translateY(-2px);
      box-shadow: 0 8px 25px rgba(0,0,0,0.1);
    }

    .subtopic-title {
      font-size: 18px;
      font-weight: 600;
      color: #7c3aed;
      margin-bottom: 10px;
    }

    .back-to-overview {
      background: linear-gradient(135deg, #a855f7, #7c3aed);
      color: white;
      border: none;
      padding: 12px 24px;
      border-radius: 25px;
      font-weight: 600;
      cursor: pointer;
      margin-bottom: 30px;
      transition: all 0.3s ease;
    }

    .back-to-overview:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 20px rgba(168, 85, 247, 0.3);
    }

    .code-example {
      background: #f8fafc;
      border: 1px solid #e2e8f0;
      border-radius: 8px;
      padding: 15px;
      margin: 10px 0;
      font-family: 'Courier New', monospace;
      font-size: 14px;
      color: #334155;
    }

    .dim-badge {
      display: inline-block;
      background: #f3e8ff;
      color: #5b21b6;
      padding: 4px 8px;
      border-radius: 12px;
      font-size: 12px;
      font-weight: 600;
      margin: 2px;
    }

    .formula-box {
      background: #faf5ff;
      border: 1px solid #d8b4fe;
      border-radius: 8px;
      padding: 15px;
      margin: 10px 0;
      font-family: 'Times New Roman', serif;
      font-size: 16px;
      text-align: center;
      color: #5b21b6;
    }

    .technique-highlight {
      background: #fef3c7;
      border: 1px solid #fbbf24;
      border-left: 4px solid #f59e0b;
      border-radius: 8px;
      padding: 15px;
      margin: 10px 0;
      font-size: 14px;
      color: #78350f;
    }

    .application-box {
      background: #f0f9ff;
      border: 1px solid #bae6fd;
      border-radius: 8px;
      padding: 12px;
      margin: 10px 0;
      font-size: 14px;
      color: #0c4a6e;
    }
  </style>
  <script>
    function showUnitDetail(unitNumber) {
      // Hide overview
      document.getElementById('overview').style.display = 'none';
      
      // Hide all unit detail pages
      const allUnits = document.querySelectorAll('.page-container');
      allUnits.forEach(unit => unit.classList.remove('active'));
      
      // Show selected unit
      const selectedUnit = document.getElementById('unit-' + unitNumber);
      if (selectedUnit) {
        selectedUnit.classList.add('active');
      }
    }

    function showOverview() {
      // Hide all unit detail pages
      const allUnits = document.querySelectorAll('.page-container');
      allUnits.forEach(unit => unit.classList.remove('active'));
      
      // Show overview
      document.getElementById('overview').style.display = 'block';
    }
  </script>
</head>
<body>
  <header>
    <h1>üìê MemoLearning Dimensionality Reduction</h1>
    <p>Reduce feature complexity while preserving essential information and patterns</p>
    <a class="back-link" href="#" onclick="alert('This would navigate back to data science courses')">‚Üê Back to Data Science</a>
  </header>

  <div class="container" id="overview">
    <div class="curriculum-stats">
      <h2 style="margin: 0 0 20px 0; font-size: 28px;">Dimensionality Reduction Curriculum</h2>
      <div class="stats-grid">
        <div class="stat-item">
          <span class="stat-number">11</span>
          <div class="stat-label">Core Units</div>
        </div>
        <div class="stat-item">
          <span class="stat-number">~70</span>
          <div class="stat-label">DR Techniques</div>
        </div>
        <div class="stat-item">
          <span class="stat-number">12+</span>
          <div class="stat-label">Algorithms</div>
        </div>
        <div class="stat-item">
          <span class="stat-number">20+</span>
          <div class="stat-label">Applications</div>
        </div>
      </div>
    </div>

    <div class="units-grid">
      <div class="unit-card" onclick="showUnitDetail(1)">
        <div class="unit-header">
          <div class="unit-number">1</div>
          <h3 class="unit-title">Introduction to Dimensionality Reduction</h3>
        </div>
        <p class="unit-description">Understand the need for dimensionality reduction and its role in machine learning and data analysis.</p>
        <ul class="topics-list">
          <li>Curse of dimensionality</li>
          <li>High-dimensional data challenges</li>
          <li>Linear vs non-linear methods</li>
          <li>Feature selection vs feature extraction</li>
          <li>Supervised vs unsupervised DR</li>
          <li>Visualization applications</li>
          <li>Computational benefits</li>
          <li>Information preservation</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(2)">
        <div class="unit-header">
          <div class="unit-number">2</div>
          <h3 class="unit-title">Principal Component Analysis (PCA)</h3>
        </div>
        <p class="unit-description">Master the most fundamental linear dimensionality reduction technique using eigenvalue decomposition.</p>
        <ul class="topics-list">
          <li>PCA mathematical foundation</li>
          <li>Covariance matrix</li>
          <li>Eigenvalues and eigenvectors</li>
          <li>Principal components</li>
          <li>Explained variance ratio</li>
          <li>Choosing number of components</li>
          <li>Data standardization</li>
          <li>PCA interpretation</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(3)">
        <div class="unit-header">
          <div class="unit-number">3</div>
          <h3 class="unit-title">Linear Discriminant Analysis (LDA)</h3>
        </div>
        <p class="unit-description">Learn supervised dimensionality reduction that maximizes class separability.</p>
        <ul class="topics-list">
          <li>LDA vs PCA comparison</li>
          <li>Between-class and within-class scatter</li>
          <li>Fisher's linear discriminant</li>
          <li>Maximizing class separation</li>
          <li>Multiclass LDA</li>
          <li>Assumptions and limitations</li>
          <li>Classification applications</li>
          <li>Regularized LDA</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(4)">
        <div class="unit-header">
          <div class="unit-number">4</div>
          <h3 class="unit-title">t-SNE and UMAP</h3>
        </div>
        <p class="unit-description">Explore advanced non-linear techniques for visualization and manifold learning.</p>
        <ul class="topics-list">
          <li>t-SNE algorithm overview</li>
          <li>Probabilistic approach</li>
          <li>Perplexity parameter</li>
          <li>UMAP principles</li>
          <li>Topological data analysis</li>
          <li>Hyperparameter tuning</li>
          <li>Visualization best practices</li>
          <li>Computational considerations</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(5)">
        <div class="unit-header">
          <div class="unit-number">5</div>
          <h3 class="unit-title">Manifold Learning</h3>
        </div>
        <p class="unit-description">Understand how to discover low-dimensional manifolds embedded in high-dimensional spaces.</p>
        <ul class="topics-list">
          <li>Manifold hypothesis</li>
          <li>Locally Linear Embedding (LLE)</li>
          <li>Isomap algorithm</li>
          <li>Multidimensional Scaling (MDS)</li>
          <li>Laplacian eigenmaps</li>
          <li>Neighborhood preservation</li>
          <li>Geodesic distances</li>
          <li>Non-linear dimensionality</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(6)">
        <div class="unit-header">
          <div class="unit-number">6</div>
          <h3 class="unit-title">Feature Selection Methods</h3>
        </div>
        <p class="unit-description">Learn techniques to select the most relevant features rather than transforming them.</p>
        <ul class="topics-list">
          <li>Filter methods</li>
          <li>Wrapper methods</li>
          <li>Embedded methods</li>
          <li>Univariate feature selection</li>
          <li>Recursive feature elimination</li>
          <li>L1 regularization</li>
          <li>Mutual information</li>
          <li>Feature importance ranking</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(7)">
        <div class="unit-header">
          <div class="unit-number">7</div>
          <h3 class="unit-title">Matrix Factorization</h3>
        </div>
        <p class="unit-description">Explore matrix decomposition techniques for dimensionality reduction and data compression.</p>
        <ul class="topics-list">
          <li>Singular Value Decomposition (SVD)</li>
          <li>Non-negative Matrix Factorization</li>
          <li>Truncated SVD</li>
          <li>Independent Component Analysis</li>
          <li>Factor analysis</li>
          <li>Matrix completion</li>
          <li>Latent factor models</li>
          <li>Recommender systems</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(8)">
        <div class="unit-header">
          <div class="unit-number">8</div>
          <h3 class="unit-title">Autoencoders</h3>
        </div>
        <p class="unit-description">Learn neural network-based approaches for non-linear dimensionality reduction.</p>
        <ul class="topics-list">
          <li>Autoencoder architecture</li>
          <li>Encoder-decoder structure</li>
          <li>Bottleneck layer</li>
          <li>Variational autoencoders</li>
          <li>Denoising autoencoders</li>
          <li>Sparse autoencoders</li>
          <li>Deep autoencoders</li>
          <li>Generative applications</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(9)">
        <div class="unit-header">
          <div class="unit-number">9</div>
          <h3 class="unit-title">Evaluation and Validation</h3>
        </div>
        <p class="unit-description">Learn methods to evaluate the quality of dimensionality reduction and choose optimal parameters.</p>
        <ul class="topics-list">
          <li>Reconstruction error</li>
          <li>Preservation of distances</li>
          <li>Neighborhood preservation</li>
          <li>Silhouette analysis</li>
          <li>Trustworthiness metrics</li>
          <li>Cross-validation strategies</li>
          <li>Downstream task performance</li>
          <li>Visual assessment</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(10)">
        <div class="unit-header">
          <div class="unit-number">10</div>
          <h3 class="unit-title">Text and Image Applications</h3>
        </div>
        <p class="unit-description">Apply dimensionality reduction techniques to specific domains like text processing and image analysis.</p>
        <ul class="topics-list">
          <li>Text preprocessing for DR</li>
          <li>TF-IDF and word embeddings</li>
          <li>Latent Semantic Analysis</li>
          <li>Topic modeling</li>
          <li>Image dimensionality reduction</li>
          <li>Face recognition applications</li>
          <li>Computer vision preprocessing</li>
          <li>Feature maps visualization</li>
        </ul>
      </div>

      <div class="unit-card" onclick="showUnitDetail(11)">
        <div class="unit-header">
          <div class="unit-number">11</div>
          <h3 class="unit-title">Practical Implementation</h3>
        </div>
        <p class="unit-description">Implement dimensionality reduction in real-world projects with best practices and optimization.</p>
        <ul class="topics-list">
          <li>Scikit-learn implementation</li>
          <li>Preprocessing pipelines</li>
          <li>Scalability considerations</li>
          <li>Memory optimization</li>
          <li>Online and incremental methods</li>
          <li>Integration with ML pipelines</li>
          <li>Performance monitoring</li>
          <li>Production deployment</li>
        </ul>
      </div>
    </div>
  </div>

  <!-- Unit Detail Pages -->
  <div class="page-container" id="unit-1">
    <button class="back-to-overview" onclick="showOverview()">‚Üê Back to Overview</button>
    <div class="unit-detail-header">
      <h1 class="unit-detail-title">Unit 1: Introduction to Dimensionality Reduction</h1>
      <p>Understand the need for dimensionality reduction and its role in machine learning and data analysis.</p>
    </div>
    <div class="subtopics-grid">
      <div class="subtopic-card">
        <h3 class="subtopic-title">Curse of Dimensionality</h3>
        <p>Learn how high-dimensional data creates challenges for machine learning algorithms and data analysis.</p>
        <span class="dim-badge">Distance Concentration</span>
        <span class="dim-badge">Sparse Data</span>
        <span class="dim-badge">Overfitting</span>
        <div class="technique-highlight">
          As dimensions increase, data points become increasingly sparse and equidistant, making pattern recognition and clustering more difficult.
        </div>
      </div>
      <div class="subtopic-card">
        <h3 class="subtopic-title">High-Dimensional Data Challenges</h3>
        <p>Understand computational and statistical problems that arise with many features.</p>
        <div class="code-example">
          import numpy as np<br>
          # Demonstrate distance concentration<br>
          # In high dimensions, distances become similar<br>
          dims = [2, 10, 50, 100]<br>
          for d in dims:<br>
          &nbsp;&nbsp;data = np.random.randn(1000, d)<br>
          &nbsp;&nbsp;distances = np.linalg.norm(data, axis=1)<br>
          &nbsp;&nbsp;print(f"Dim {d}: std/mean = {distances.std()/distances.mean():.3f}")
        </div>
      </div>
      <div class="subtopic-card">
        <h3 class="subtopic-title">Linear vs Non-linear Methods</h3>
        <p>Compare linear transformations like PCA with non-linear methods like t-SNE.</p>
        <div class="application-box">
          Linear methods: PCA, LDA, Factor Analysis<br>
          Non-linear methods: t-SNE, UMAP, Autoencoders<br>
          Choose based on data structure and goals
        </div>
        <div class="code-example">
          from sklearn.decomposition import PCA<br>
          from sklearn.manifold import TSNE<br>
          <br>
          # Linear reduction<br>
          pca = PCA(n_components=2)<br>
          X_pca = pca.fit_transform(X)<br>
          <br>
          # Non-linear reduction<br>
          tsne = TSNE(n_components=2)<br>
          X_tsne = tsne.fit_transform(X)
        </div>
      </div>
      <div class="subtopic-card">
        <h3 class="subtopic-title">Feature Selection vs Feature Extraction</h3>
        <p>Distinguish between selecting existing features and creating new transformed features.</p>
        <div class="formula-box">
          Feature Selection: Choose subset of original features<br>
          Feature Extraction: Create new features as combinations
        </div>
        <div class="code-example">
          from sklearn.feature_selection import SelectKBest<br>
          from sklearn.decomposition import PCA<br>
          <br>
          # Feature selection - keeps original features<br>
          selector = SelectKBest(k=5)<br>
          X_selected = selector.fit_transform(X, y)<br>
          <br>
          # Feature extraction - creates new features<br>
          pca = PCA(n_components=5)<br>
          X_extracted = pca.fit_transform(X)
        </div>
      </div>
      <div class="subtopic-card">
        <h3 class="subtopic-title">Supervised vs Unsupervised DR</h3>
        <p>Learn when to use methods that consider target variables versus those that don't.</p>
        <span class="dim-badge">Supervised: LDA</span>
        <span class="dim-badge">Unsupervised: PCA</span>
        <div class="code-example">
          # Unsupervised - doesn't use labels<br>
          pca = PCA(n_components=2)<br>
          X_pca = pca.fit_transform(X)  # Only X, no y<br>
          <br>
          # Supervised - uses labels for better separation<br>
          from sklearn.discriminant_analysis import LinearDiscriminantAnalysis<br>
          lda = LinearDiscriminantAnalysis(n_components=2)<br>
          X_lda = lda.fit_transform(X, y)  # Uses both X and y
        </div>
      </div>
      <div class="subtopic-card">
        <h3 class="subtopic-title">Visualization Applications</h3>
        <p>Use dimensionality reduction to create 2D and 3D visualizations of high-dimensional data.</p>
        <div class="code-example">
          import matplotlib.pyplot as plt<br>
          <br>
          # Visualize high-dimensional data in 2D<br>
          plt.figure(figsize=(12, 5))<br>
          <br>
          plt.subplot(1, 2, 1)<br>
          plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')<br>
          plt.title('PCA Visualization')<br>
          <br>
          plt.subplot(1, 2, 2)<br>
          plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')<br>
          plt.title('t-SNE Visualization')
        </div>
      </div>
      <div class="subtopic-card">
        <h3 class="subtopic-title">Computational Benefits</h3>
        <p>Understand how dimensionality reduction improves computational efficiency and storage requirements.</p>
        <div class="technique-highlight">
          ‚Ä¢ Faster training and prediction<br>
          ‚Ä¢ Reduced memory usage<br>
          ‚Ä¢ Lower storage requirements<br>
          ‚Ä¢ Improved numerical stability<br>
          ‚Ä¢ Noise reduction
        </div>
      </div>
      <div class="subtopic-card">
        <h3 class="subtopic-title">Information Preservation</h3>
        <p>Learn to balance dimensionality reduction with preserving important information in the data.</p>
        <div class="code-example">
          # Check how much variance is preserved<br>
          pca = PCA()<br>
          pca.fit(X)<br>
          <br>
          # Cumulative explained variance<br>
          cumsum = np.cumsum(pca.explained_variance_ratio_)<br>
          n_components = np.argmax(cumsum >= 0.95) + 1<br>
          print(f"Need {n_components} components for 95% variance")
        </div>
      </div>
    </div>
  </div>

  <div class="page-container" id="unit-2">
    <button class="back-to-overview" onclick="showOverview()">‚Üê Back to Overview</button>
    <div class="unit-detail-header">
      <h1 class="unit-detail-title">Unit 2: Principal Component Analysis (PCA)</h1>
      <p>Master the most fundamental linear dimensionality reduction technique using eigenvalue decomposition.</p>
    </div>
    <div class="subtopics-grid">
      <div class="subtopic-card">
        <h3 class="subtopic-title">PCA Mathematical Foundation</h3>
        <p>Understand the mathematical principles behind PCA and how it finds principal components.</p>
        <div class="formula-box">
          PCA finds directions of maximum variance:<br>
          C = (1/n)X^T X (covariance matrix)<br>
          Cv = Œªv (eigenvalue equation)
        </div>
        <div class="code-example">
          import numpy as np<br>
          from sklearn.decomposition import PCA<br>
          <br>
          # Manual PCA computation<br>
          X_centered = X - np.mean(X, axis=0)<br>
          cov_matrix = np.cov(X_centered.T)<br>
          eigenvals, eigenvecs = np.linalg.eig(cov_matrix)
        </div>
      </div
